{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Data Preperation Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:03.667630Z",
     "start_time": "2020-04-29T11:40:03.663429Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:04.044904Z",
     "start_time": "2020-04-29T11:40:03.861034Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = pd.read_csv('https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv', \n",
    "                      delimiter = '\\t',\n",
    "                      quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* print top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:04.238480Z",
     "start_time": "2020-04-29T11:40:04.226793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1\n",
       "5     Now I am getting angry and I want my damn pho.      0\n",
       "6              Honeslty it didn't taste THAT fresh.)      0\n",
       "7  The potatoes were like rubber and you could te...      0\n",
       "8                          The fries were great too.      1\n",
       "9                                     A great touch.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from the URL\n",
    "url = 'https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv'\n",
    "dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Display the top 10 rows\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function that removes punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:04.593691Z",
     "start_time": "2020-04-29T11:40:04.588163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                               Wow Loved this place      1\n",
      "1                                  Crust is not good      0\n",
      "2           Not tasty and the texture was just nasty      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n",
      "5      Now I am getting angry and I want my damn pho      0\n",
      "6                 Honeslty it didnt taste THAT fresh      0\n",
      "7  The potatoes were like rubber and you could te...      0\n",
      "8                           The fries were great too      1\n",
      "9                                      A great touch      1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function that converts text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:04.953719Z",
     "start_time": "2020-04-29T11:40:04.949990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                               wow loved this place      1\n",
      "1                                  crust is not good      0\n",
      "2           not tasty and the texture was just nasty      0\n",
      "3  stopped by during the late may bank holiday of...      1\n",
      "4  the selection on the menu was great and so wer...      1\n",
      "5      now i am getting angry and i want my damn pho      0\n",
      "6                 honeslty it didnt taste that fresh      0\n",
      "7  the potatoes were like rubber and you could te...      0\n",
      "8                           the fries were great too      1\n",
      "9                                      a great touch      1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Apply the functions to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "dataset['Review'] = dataset['Review'].apply(to_lowercase)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import nltk and download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.172105Z",
     "start_time": "2020-04-29T11:40:05.315632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                    wow loved place      1\n",
      "1                                         crust good      0\n",
      "2                                tasty texture nasty      0\n",
      "3  stopped late may bank holiday rick steve recom...      1\n",
      "4                        selection menu great prices      1\n",
      "5                        getting angry want damn pho      0\n",
      "6                         honeslty didnt taste fresh      0\n",
      "7  potatoes like rubber could tell made ahead tim...      0\n",
      "8                                        fries great      1\n",
      "9                                        great touch      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the functions to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "dataset['Review'] = dataset['Review'].apply(to_lowercase)\n",
    "dataset['Review'] = dataset['Review'].apply(remove_stopwords)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function that removes stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.235109Z",
     "start_time": "2020-04-29T11:40:07.225647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                    wow loved place      1\n",
      "1                                         crust good      0\n",
      "2                                tasty texture nasty      0\n",
      "3  stopped late may bank holiday rick steve recom...      1\n",
      "4                        selection menu great prices      1\n",
      "5                        getting angry want damn pho      0\n",
      "6                         honeslty didnt taste fresh      0\n",
      "7  potatoes like rubber could tell made ahead tim...      0\n",
      "8                                        fries great      1\n",
      "9                                        great touch      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Apply the functions to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "dataset['Review'] = dataset['Review'].apply(to_lowercase)\n",
    "dataset['Review'] = dataset['Review'].apply(remove_stopwords)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import Lemmatizer from nltk and download wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.325675Z",
     "start_time": "2020-04-29T11:40:07.286398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                    wow loved place      1\n",
      "1                                         crust good      0\n",
      "2                                tasty texture nasty      0\n",
      "3  stopped late may bank holiday rick steve recom...      1\n",
      "4                         selection menu great price      1\n",
      "5                        getting angry want damn pho      0\n",
      "6                         honeslty didnt taste fresh      0\n",
      "7  potato like rubber could tell made ahead time ...      0\n",
      "8                                          fry great      1\n",
      "9                                        great touch      1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
    "\n",
    "# Apply the functions to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "dataset['Review'] = dataset['Review'].apply(to_lowercase)\n",
    "dataset['Review'] = dataset['Review'].apply(remove_stopwords)\n",
    "dataset['Review'] = dataset['Review'].apply(lemmatize_text)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function that lemmatizes text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.408715Z",
     "start_time": "2020-04-29T11:40:07.397412Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import Stemming from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.467033Z",
     "start_time": "2020-04-29T11:40:07.457053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                     wow love place      1\n",
      "1                                         crust good      0\n",
      "2                                 tasti textur nasti      0\n",
      "3  stop late may bank holiday rick steve recommen...      1\n",
      "4                            select menu great price      1\n",
      "5                            get angri want damn pho      0\n",
      "6                          honeslti didnt tast fresh      0\n",
      "7  potato like rubber could tell made ahead time ...      0\n",
      "8                                          fri great      1\n",
      "9                                        great touch      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer and Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
    "\n",
    "# Function to stem the text\n",
    "def stem_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]  # Stem each word\n",
    "    return ' '.join(stemmed_words)  # Join the stemmed words back into a single string\n",
    "\n",
    "# Apply the functions to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(remove_punctuation)\n",
    "dataset['Review'] = dataset['Review'].apply(to_lowercase)\n",
    "dataset['Review'] = dataset['Review'].apply(remove_stopwords)\n",
    "dataset['Review'] = dataset['Review'].apply(lemmatize_text)\n",
    "dataset['Review'] = dataset['Review'].apply(stem_text)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function thath stems text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.516026Z",
     "start_time": "2020-04-29T11:40:07.509217Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function `'preprocess'` that applies all the preprocessing steps we defined above, **without** Stemming and Lemmatizing, to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:07.579774Z",
     "start_time": "2020-04-29T11:40:07.562065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                     wow love place      1\n",
      "1                                         crust good      0\n",
      "2                                 tasti textur nasti      0\n",
      "3  stop late may bank holiday rick steve recommen...      1\n",
      "4                            select menu great price      1\n",
      "5                            get angri want damn pho      0\n",
      "6                          honeslti didnt tast fresh      0\n",
      "7  potato like rubber could tell made ahead time ...      0\n",
      "8                                          fri great      1\n",
      "9                                        great touch      1\n"
     ]
    }
   ],
   "source": [
    "# Define the 'preprocess' function to apply all steps except stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply the 'preprocess' function to the dataset\n",
    "dataset['Review'] = dataset['Review'].apply(preprocess)\n",
    "\n",
    "# Display the top 10 rows\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a new column in dataset with name 'Review_clean' and store the preprocessed reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:09.529347Z",
     "start_time": "2020-04-29T11:40:07.810861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>wow loved place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>crust good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>tasty texture nasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>selection menu great prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>getting angry want damn pho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>honeslty didnt taste fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>potatoes like rubber could tell made ahead tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>fries great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>great touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0                           Wow... Loved this place.   \n",
       "1                                 Crust is not good.   \n",
       "2          Not tasty and the texture was just nasty.   \n",
       "3  Stopped by during the late May bank holiday of...   \n",
       "4  The selection on the menu was great and so wer...   \n",
       "5     Now I am getting angry and I want my damn pho.   \n",
       "6              Honeslty it didn't taste THAT fresh.)   \n",
       "7  The potatoes were like rubber and you could te...   \n",
       "8                          The fries were great too.   \n",
       "9                                     A great touch.   \n",
       "\n",
       "                                        Review_clean  \n",
       "0                                    wow loved place  \n",
       "1                                         crust good  \n",
       "2                                tasty texture nasty  \n",
       "3  stopped late may bank holiday rick steve recom...  \n",
       "4                        selection menu great prices  \n",
       "5                        getting angry want damn pho  \n",
       "6                         honeslty didnt taste fresh  \n",
       "7  potatoes like rubber could tell made ahead tim...  \n",
       "8                                        fries great  \n",
       "9                                        great touch  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Define the 'preprocess' function to apply all steps except stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Simulate loading the dataset again\n",
    "url = 'https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv'\n",
    "dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Apply the 'preprocess' function to create a new column 'Review_clean'\n",
    "dataset['Review_clean'] = dataset['Review'].apply(preprocess)\n",
    "\n",
    "# Display the top 10 rows with the new 'Review_clean' column\n",
    "dataset[['Review', 'Review_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* print top 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:09.595017Z",
     "start_time": "2020-04-29T11:40:09.579025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                        Review_clean  \n",
      "0                                    wow loved place  \n",
      "1                                         crust good  \n",
      "2                                tasty texture nasty  \n",
      "3  stopped late may bank holiday rick steve recom...  \n",
      "4                        selection menu great prices  \n",
      "5                        getting angry want damn pho  \n",
      "6                         honeslty didnt taste fresh  \n",
      "7  potatoes like rubber could tell made ahead tim...  \n",
      "8                                        fries great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Define the 'preprocess' function to apply all steps except stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "url = 'https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv'\n",
    "dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Apply the 'preprocess' function to create a new column 'Review_clean'\n",
    "dataset['Review_clean'] = dataset['Review'].apply(preprocess)\n",
    "\n",
    "# Display the top 10 rows of the dataset with both 'Review' and 'Review_clean' columns\n",
    "print(dataset[['Review', 'Review_clean']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply the lemmatizer function above on the 'Review_clean' column of the dataset.\n",
    "* store the result in a 'Lemmatized' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:11.380853Z",
     "start_time": "2020-04-29T11:40:09.630497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                        Review_clean  \\\n",
      "0                                    wow loved place   \n",
      "1                                         crust good   \n",
      "2                                tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...   \n",
      "4                        selection menu great prices   \n",
      "5                        getting angry want damn pho   \n",
      "6                         honeslty didnt taste fresh   \n",
      "7  potatoes like rubber could tell made ahead tim...   \n",
      "8                                        fries great   \n",
      "9                                        great touch   \n",
      "\n",
      "                                          Lemmatized  \n",
      "0                                    wow loved place  \n",
      "1                                         crust good  \n",
      "2                                tasty texture nasty  \n",
      "3  stopped late may bank holiday rick steve recom...  \n",
      "4                         selection menu great price  \n",
      "5                        getting angry want damn pho  \n",
      "6                         honeslty didnt taste fresh  \n",
      "7  potato like rubber could tell made ahead time ...  \n",
      "8                                          fry great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
    "\n",
    "# Define the 'preprocess' function to apply all steps except stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "url = 'https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv'\n",
    "dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Apply the 'preprocess' function to create a new column 'Review_clean'\n",
    "dataset['Review_clean'] = dataset['Review'].apply(preprocess)\n",
    "\n",
    "# Apply the lemmatizer function to the 'Review_clean' column and store the result in 'Lemmatized' column\n",
    "dataset['Lemmatized'] = dataset['Review_clean'].apply(lemmatize_text)\n",
    "\n",
    "# Display the top 10 rows with 'Review', 'Review_clean', and 'Lemmatized' columns\n",
    "print(dataset[['Review', 'Review_clean', 'Lemmatized']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* print top 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:11.433810Z",
     "start_time": "2020-04-29T11:40:11.421811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                        Review_clean  \\\n",
      "0                                    wow loved place   \n",
      "1                                         crust good   \n",
      "2                                tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...   \n",
      "4                        selection menu great prices   \n",
      "5                        getting angry want damn pho   \n",
      "6                         honeslty didnt taste fresh   \n",
      "7  potatoes like rubber could tell made ahead tim...   \n",
      "8                                        fries great   \n",
      "9                                        great touch   \n",
      "\n",
      "                                          Lemmatized  \n",
      "0                                    wow loved place  \n",
      "1                                         crust good  \n",
      "2                                tasty texture nasty  \n",
      "3  stopped late may bank holiday rick steve recom...  \n",
      "4                         selection menu great price  \n",
      "5                        getting angry want damn pho  \n",
      "6                         honeslty didnt taste fresh  \n",
      "7  potato like rubber could tell made ahead time ...  \n",
      "8                                          fry great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 rows of the dataset with 'Review', 'Review_clean', and 'Lemmatized' columns\n",
    "print(dataset[['Review', 'Review_clean', 'Lemmatized']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply stemmer defined above on 'Review_clean' column of the dataset.\n",
    "* store the result in 'Stemmed' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:11.677728Z",
     "start_time": "2020-04-29T11:40:11.472960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\a7902\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                        Review_clean  \\\n",
      "0                                    wow loved place   \n",
      "1                                         crust good   \n",
      "2                                tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...   \n",
      "4                        selection menu great prices   \n",
      "5                        getting angry want damn pho   \n",
      "6                         honeslty didnt taste fresh   \n",
      "7  potatoes like rubber could tell made ahead tim...   \n",
      "8                                        fries great   \n",
      "9                                        great touch   \n",
      "\n",
      "                                             Stemmed  \n",
      "0                                     wow love place  \n",
      "1                                         crust good  \n",
      "2                                 tasti textur nasti  \n",
      "3  stop late may bank holiday rick steve recommen...  \n",
      "4                            select menu great price  \n",
      "5                            get angri want damn pho  \n",
      "6                          honeslti didnt tast fresh  \n",
      "7  potato like rubber could tell made ahead time ...  \n",
      "8                                          fri great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer and Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove punctuation from a given text\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a single string\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
    "\n",
    "# Function to stem the text\n",
    "def stem_text(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]  # Stem each word using PorterStemmer\n",
    "    return ' '.join(stemmed_words)  # Join the stemmed words back into a single string\n",
    "\n",
    "# Define the 'preprocess' function to apply all steps except stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "url = 'https://media.geeksforgeeks.org/wp-content/uploads/Restaurant_Reviews.tsv'\n",
    "dataset = pd.read_csv(url, delimiter='\\t', quoting=3)\n",
    "\n",
    "# Apply the 'preprocess' function to create a new column 'Review_clean'\n",
    "dataset['Review_clean'] = dataset['Review'].apply(preprocess)\n",
    "\n",
    "# Apply the stemmer function to the 'Review_clean' column and store the result in 'Stemmed' column\n",
    "dataset['Stemmed'] = dataset['Review_clean'].apply(stem_text)\n",
    "\n",
    "# Display the top 10 rows with 'Review', 'Review_clean', and 'Stemmed' columns\n",
    "print(dataset[['Review', 'Review_clean', 'Stemmed']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* print top 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:40:11.779253Z",
     "start_time": "2020-04-29T11:40:11.742611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                        Review_clean  \\\n",
      "0                                    wow loved place   \n",
      "1                                         crust good   \n",
      "2                                tasty texture nasty   \n",
      "3  stopped late may bank holiday rick steve recom...   \n",
      "4                        selection menu great prices   \n",
      "5                        getting angry want damn pho   \n",
      "6                         honeslty didnt taste fresh   \n",
      "7  potatoes like rubber could tell made ahead tim...   \n",
      "8                                        fries great   \n",
      "9                                        great touch   \n",
      "\n",
      "                                             Stemmed  \n",
      "0                                     wow love place  \n",
      "1                                         crust good  \n",
      "2                                 tasti textur nasti  \n",
      "3  stop late may bank holiday rick steve recommen...  \n",
      "4                            select menu great price  \n",
      "5                            get angri want damn pho  \n",
      "6                          honeslti didnt tast fresh  \n",
      "7  potato like rubber could tell made ahead time ...  \n",
      "8                                          fri great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 rows of the dataset with 'Review', 'Review_clean', and 'Stemmed' columns\n",
    "print(dataset[['Review', 'Review_clean', 'Stemmed']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function `'preprocess_all'` that applies all the preprocessing steps we defined above, **with** Stemming and Lemmatizing, to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'preprocess_all' function to apply all preprocessing steps with both stemming and lemmatizing\n",
    "def preprocess_all(text):\n",
    "    text = remove_punctuation(text)  # Remove punctuation\n",
    "    text = to_lowercase(text)        # Convert to lowercase\n",
    "    text = remove_stopwords(text)    # Remove stopwords\n",
    "    text = lemmatize_text(text)      # Lemmatize the words\n",
    "    text = stem_text(text)           # Apply stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0                           Wow... Loved this place.   \n",
      "1                                 Crust is not good.   \n",
      "2          Not tasty and the texture was just nasty.   \n",
      "3  Stopped by during the late May bank holiday of...   \n",
      "4  The selection on the menu was great and so wer...   \n",
      "5     Now I am getting angry and I want my damn pho.   \n",
      "6              Honeslty it didn't taste THAT fresh.)   \n",
      "7  The potatoes were like rubber and you could te...   \n",
      "8                          The fries were great too.   \n",
      "9                                     A great touch.   \n",
      "\n",
      "                                           Processed  \n",
      "0                                     wow love place  \n",
      "1                                         crust good  \n",
      "2                                 tasti textur nasti  \n",
      "3  stop late may bank holiday rick steve recommen...  \n",
      "4                            select menu great price  \n",
      "5                            get angri want damn pho  \n",
      "6                          honeslti didnt tast fresh  \n",
      "7  potato like rubber could tell made ahead time ...  \n",
      "8                                          fri great  \n",
      "9                                        great touch  \n"
     ]
    }
   ],
   "source": [
    "# Apply the 'preprocess_all' function to the 'Review' column and store the result in a new column 'Processed'\n",
    "dataset['Processed'] = dataset['Review'].apply(preprocess_all)\n",
    "\n",
    "# Display the top 10 rows of the dataset with 'Review', 'Processed' columns\n",
    "print(dataset[['Review', 'Processed']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THE_ONE",
   "language": "python",
   "name": "the_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
